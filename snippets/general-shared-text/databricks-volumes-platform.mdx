Fill in the following fields:

- **Name** (_required_): A unique name for this connector.
- **Host** (_required_): The Databricks workspace host URL.
- **Cluster ID** : The Databricks cluster ID.
- **Catalog** (_required_): The name of the catalog to use.
- **Schema** : The name of the associated schema. If not specified, **default** is used.
- **Volume** (_required_): The name of the associated volume.
- **Volume Path** : Any optional path to access within the volume.
- **Overwrite** Check this box if existing data should be overwritten.
- **Encoding** : Any encoding to be applied to the data in the volume. If not specified, **utf-8**, is used. 

Also fill in the following fields based on your authentication type, depending on your cloud provider:

- For Databricks personal access token authentication (AWS, Azure, and GCP):

  - **Token** : The Databricks personal access token value.

- For OAuth machine-to-machine (M2M) authentication (AWS, Azure, and GCP):

  - **Client ID** : The Databricks client ID value for the corresponding service principal.
  - **Client Secret** : The associated Databricks service principal OAuth secret.

The following authentication types are currently not supported:

- Username and password (basic) authentication (AWS only).
- OAuth user-to-machine (U2M) authentication (AWS, Azure, and GCP).
- Azure managed identities (MSI) authentication (Azure only).
- Microsoft Entra ID service principal authentication (Azure only).
- Azure CLI authentication (Azure only).
- Microsoft Entra ID user authentication (Azure only).
- Google Cloud Platform credentials authentication (GCP only).
- Google Cloud Platform ID authentication (GCP only).



