Connect S3 to your preprocessing pipeline, and use the Unstructured CLI or Python SDK to batch process all your documents and store structured outputs locally on your filesystem.

You will need: 

- The S3 connector dependencies:

  ```bash
  pip install "unstructured[s3]"
  ```

- These environment variables:

  - `AWS_S3_URL` - The S3 remote URL, formatted as `protocol://dir/path/` (for example, `s3://my-bucket/my-folder/`).
  - If your S3 bucket doesn't allow anonymous access, provide your AWS credentials:

    - `--key` (CLI) or `key` (Python SDK) - The AWS access key ID, represented as `AWS_ACCESS_KEY_ID`.
    - `--secret` (CLI) or `secret` (Python SDK) - Your AWS secret access key, represented as `AWS_SECRET_ACCESS_KEY`.

    If your S3 bucket allows access without local AWS credentials, set `--anonymous` (CLI) or `anonymous=True` (Python SDK) instead.