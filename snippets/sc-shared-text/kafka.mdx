Connect Kafka/Confluent to your preprocessing pipeline, and batch process all your documents using ``unstructured-ingest`` to store structured outputs locally on your filesystem.

First youâ€™ll need to install the Kafka dependencies as shown here.

```bash
pip install "unstructured[kafka]"
```
To processes all the issues in projects within your Jira domain, you must specify:
* `url`:Atlassian (Jira) domain URL
* `api-token`: API token to authenticate into Atlassian (Jira). Check [Atlassian documentation](https://support.atlassian.com/atlassian-account/docs/manage-api-tokens-for-your-atlassian-account/) for more info.
* `user-email`: User email for the domain

Optionally, you can limit what issues should be processed:
* `list-of-projects`: explicitly specify which project ids should be processed
* `list-of-boards`: explicitly specify which board ids should be processed
* `list-of-issues`: explicitly specify which issue ids or keys should be processed

If any of the optional arguments are provided, connector will ingest only those components, and nothing else.
When none of the optional arguments are provided, all issues in all projects will be ingested.
