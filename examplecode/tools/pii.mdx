---
title: PII detection
---

Personally identifiable information (PII) detection is important as part of an organization's overall strategy for 
minimizing potential harm from misuse or unauthorized access to individual's private data. Detecting PII is a first step 
toward helping avoid identity theft, maintaining privacy, and building trust with customers and users. Organizations 
might aslo need to follow various data protection regulations, making PII detection a crucial part of the 
organization's legal compliance framework.

This hands-on example walkthrough demonstrates how to use [Microsoft Presidio](https://microsoft.github.io/presidio/) 
to identify PII, and then anonymize the identified PII within, Unstructured JSON output files. Presidio can identify 
and anonymize entities in text and images such as credit card numbers, names, locations, social security numbers, 
bitcoin wallets, US phone numbers, financial data, and more.

In this walkthrough, you will use Python code to loop through a folder within an Amazon S3 bucket that contains a 
collection of Unstructured JSON output files. For each file, your code will use Presidio to identify PII that matches 
specific patterns and anonymize the identified PII. Your code will then write the Unstrucutred JSON output files' contents 
that contain the anonymized PII to a separate folder within an S3 bucket. You can then compare the Unstructured 
JSON output files' original content with the anonymized content to see how Presidio performed. 

## Requirements

import GetStartedSimpleUIOnly from '/snippets/general-shared-text/get-started-simple-ui-only.mdx'

To use this example, you will need:

- An Unstructured account, as follows:

  <GetStartedSimpleUIOnly />

- A set of one or more JSON output files that have been generated by Unstructured and stored in a folder within an 
  Amazon S3 bucket that you have access to. One way to generate these files is to use an Unstructured workflow that 
  relies on an S3 destination connector. Learn how to [create an S3 destination connector](/ui/destinations/s3) and 
  [create a custom workflow](/ui/workflows#create-a-custom-workflow) that uses your S3 destination connector.
- Python installed on your local development machine. 

## Step 1: Create a Python script

1. In your local Python virtual environment, install the following libraries:

   - `boto3`
   - `presidio_analyzer`
   - `presidio_anonymizer`

   For example, if you are using [uv](https://docs.astral.sh/uv/), you can install the libraries into your local `uv` 
   virtual environment with the following command:

   ```bash
   uv add boto3 presidio_analyzer presidio_anonymizer
   ```

2. In your local Python virtual environment, install ...

   ... See https://spacy.io/models for models ...

   ```basH# 
   uv pip install en_core_web_lg@https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl
   ```

3. [Set up Boto3 credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for your AWS account. 
   The following steps assume you have set up your Boto3 credentials from outside of the following code, such as setting 
   [environment variables](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#environment-variables) or 
   configuring a [shared credentials file](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#shared-credentials-file), 

   One approach to getting and setting up Boto3 credentials is to [create an AWS access key and secret access key](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey) 
   and then use the [AWS Command Line Interface (AWS CLI)](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) 
   to [set up your credentials](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html#cli-configure-files-methods) on your local development machine.

   <iframe
   width="560"
   height="315"
   src="https://www.youtube.com/embed/MoFTaGJE65Q"
   title="YouTube video player"
   frameborder="0"
   allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
   allowfullscreen
   ></iframe>

4. Add the following code to a Python script file in your virtual environment, replacing the following placeholders:

   - Replace `<input-bucket-name>` with the name of the Amazon S3 bucket that contains your original Unstructured JSON output files.
   - Replace `<input-folder-prefix>` with the path to the folder within the input bucket that contains your original Unstructured JSON output files.
   - Replace `<output-bucket-name>` with the name of the S3 bucket that will contain copies of the contents of your Unstructured JSON output files, 
     with the anonymized content within those files' copies. This can be the same bucket as the input bucket, or a different bucket.
   - Replace `<output-folder-prefix>` with the path to the folder within the output bucket that will contain copies of the contents of your Unstructured 
     JSON output files, with the anonymized content within those files' copies. This must not be the same folder as the input folder.
   - Replace `<bucket-region-short-id>` with the short ID of the region where your buckets are located, for example `us-east-1`.
   
   For the `operators` variable, a list of operators for built-in Presidio entities is specified. These operators look for common entities such as 
   credit card numbers, email addresses, phone numbers, and more. You can remove any entities from this list that you do not 
   want your code to look for. You can also add operators to this list for additional 
   [built-in entities](https://microsoft.github.io/presidio/supported_entities/) that you want your code to also look for. And you can also 
   [add your own custom entities](https://microsoft.github.io/presidio/analyzer/adding_recognizers/) to this list.

   ```python
   import boto3
   import json

   from presidio_analyzer import AnalyzerEngine
   from presidio_anonymizer import AnonymizerEngine
   from presidio_anonymizer.entities import OperatorConfig

   operators={
       "CREDIT_CARD": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_CREDIT_CARD>'}),
       "CRYPTO": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_CRYPTO>'}),
       "EMAIL_ADDRESS": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_EMAIL_ADDRESS>'}),
       "IBAN_CODE": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_IBAN_CODE>'}),
       "IP_ADDRESS": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_IP_ADDRESS>'}),
       "NRP": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_NRP>'}),
       "LOCATION": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_LOCATION>'}), 
       "PERSON": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_PERSON>'}),
       "PHONE_NUMBER": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_PHONE_NUMBER>'}),
       "MEDICAL_LICENSE": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_MEDICAL_LICENSE>'}),
       "URL": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_URL>'}),
       "US_BANK_NUMBER": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_US_BANK_NUMBER>'}),
       "US_DRIVER_LICENSE": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_US_DRIVER_LICENSE>'}),
       "US_ITIN": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_US_ITIN>'}),
       "US_PASSPORT": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_US_PASSPORT>'}),
       "US_SSN": OperatorConfig(operator_name="replace", params={'new_value': '<ANONYMIZED_US_SSN>'})
   }

   # Recursively check for string values in the provided JSON object (in this case, 
   # the "metadata" field of the JSON object) and anonymize them 
   # as appropriate.
   def check_string_values(obj, analyzer, anonymizer):
       if isinstance(obj, dict):
           for key, value in obj.items():
               # Skip analyzing Base64-encoded image fields.
               if key == 'image_base64':
                   pass
               elif isinstance(value, str):
                   anonymized_results = anonymizer.anonymize(
                       text=value,
                       analyzer_results=analyzer.analyze(text=value, language="en"),
                       operators=operators
                   )
                   value = anonymized_results.text
               # Recurse through nested "metadata" fields.
               elif isinstance(value, dict):
                   check_string_values(value, analyzer, anonymizer)
               # Skip analyzing non-string fields.
               else:
                   pass
       return obj

   def main():
       s3_input_bucket_name    = '<input-bucket-name>'
       s3_input_folder_prefix  = '<input-folder-prefix>'
       s3_output_bucket_name   = '<output-bucket-name>'
       s3_output_folder_prefix = '<output-folder-prefix>'
       s3_bucket_region        = '<bucket-region-short-id>'

       s3_client = boto3.client('s3')

       # Load the JSON files from the input folder.
       # Normalize the input folderprefix to ensure it ends with '/'.
       if not s3_input_folder_prefix.endswith('/'):
           s3_input_folder_prefix += '/'

       paginator = s3_client.get_paginator('list_objects_v2')
       page_iterator = paginator.paginate(
           Bucket=s3_input_bucket_name, 
           Prefix=s3_input_folder_prefix
       )
       files = []

       # Get the list of file keys from the input folder to anaylyze.
       # A file's key is the full path to the file within the bucket.
       # For example, if the input folder's name is "original" and the 
       # input file's name is "file1.json", the file's key is 
       # "original/file1.json".
       # There could be multiple "pages" of file listings available, 
       # so each of these "pages" must be looped through, so that 
       # no files are missed.
       for page in page_iterator:
           # 'Contents' is missing if the folder is empty or the 
           # intended prefix is not found.
           if 'Contents' in page:
               for obj in page['Contents']:
                   key = obj['Key']
                   if not key.endswith('/'):  # Skip if it's a folder placeholder.
                       files.append(key)

       analyzer = AnalyzerEngine()
       anonymizer = AnonymizerEngine()
       s3_resource = boto3.resource('s3')

       # For each JSON file to analyze, load the JSON data.
       for key in files:
           content_object = s3_resource.Object(
               bucket_name=s3_input_bucket_name, 
               key=key 
           )

           file_content = content_object.get()['Body'].read().decode('utf-8')  # Bytes.
           json_data = json.loads(file_content) # Test to JSON.

           # For each element in the JSON data...
           for element in json_data:
               # If there is a "text" field...
               if 'text' in element:
                   # ...get the text content...
                   text_element = element['text']
                   # ...and analyze and anonymize the text content as appropriate.
                   anonymized_results = anonymizer.anonymize(
                       text=text_element,
                       analyzer_results=analyzer.analyze(text=text_element, language="en"),
                       operators=operators
                   )
                   element['text'] = anonymized_results.text
               # If there is a "metadata" field...
               if 'metadata' in element:
                   # ...get the metadata content...
                   metadata_element = element['metadata']
                   # ...and analyze and anonymize the metadata content as appropriate.
                   element['metadata'] = check_string_values(metadata_element, analyzer, anonymizer)

           # Get the filename from the key.
           filename = key.split(s3_input_folder_prefix)[1]

           # Normalize the output folder prefix to ensure it ends with '/'.
           if not s3_output_folder_prefix.endswith('/'):
               s3_output_folder_prefix += '/'
           
           # Then save the JSON data with its anonymizations to the destination folder.
           s3_client.put_object(
               Bucket=s3_output_bucket_name,
               Key=f"{s3_output_folder_prefix}{filename}",
               Body=json.dumps(obj=json_data, indent=4).encode('utf-8')
           )

   if __name__ == "__main__":
       main()
   ```