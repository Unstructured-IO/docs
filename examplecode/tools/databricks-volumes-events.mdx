---
title: Databricks Volumes events
---

You can use Databricks Volumes events, such as adding new files to&mdash;or updating existing files within&mdash;Databricks Volumes, to automatically run Unstructured ETL+ workflows 
that rely on those Databricks Volumes as sources. This enables a no-touch approach to having Unstructured automatically process new and updated files in Databricks Volumes as they are added or updated.

This example shows how to automate this process by adding a custom job in Lakeflow Jobs for your Databricks workspace in 
[AWS](https://docs.databricks.com/aws/jobs/), [Azure](https://learn.microsoft.com/azure/databricks/jobs/), or 
[GCP](https://docs.databricks.com/gcp/jobs). This job runs 
whenever a new or updated file is detected in the specified Databricks Volume. This job then calls the [Unstructured Workflow Endpoint](/api-reference/workflow/overview) to automatically run the 
specified corresponding Unstructured ETL+ workflow within your Unstructured account.

<Note>
    This example uses a custom job in Lakeflow Jobs that you create and maintain. 
    Any issues with file detection, timing, or job execution could be related to your custom job, 
    rather than with Unstructured. If you are getting unexpected or no results, be sure to check your custom 
    job's execution logs first for any informational and error messages.
</Note>

## Requirements

import GetStartedSimpleApiOnly from '/snippets/general-shared-text/get-started-simple-api-only.mdx'

To use this example, you will need the following:

- An Unstructured account, and an Unstructured API key for your account, as follows:

  <GetStartedSimpleApiOnly />

- The Unstructured Workflow Endpoint URL for your account, as follows:

  1. In the Unstructured UI, click **API Keys** on the sidebar.<br/>
  2. Note the value of the **Unstructured Workflow Endpoint** field.

- A Databricks Volumes source connector in your Unstructured account. [Learn how](/ui/sources/databricks-volumes). 
- Some available [destination connector](/ui/destinations/overview) in your Unstructured account.
- A workflow that uses the preceding source and destination connectors. [Learn how](/ui/workflows).

## Step 1: Create a notebook to run the Unstructuredworkflow

1. Sign in to the Databricks workspace within your Databricks account for AWS, Azure, or GCP that 
   corresponds to the workspace you specified for your Databricks Volumes source connector.
2. On the sidebar, click **+ New > Notebook**.
3. Click the notebook's title and change it to something more descriptive, such as `Unstructured Workflow Runner`.
4. In the notebook's first cell, add the following code:

   ```python
   ...
   ```

5. Click **Edit > Insert cell below**.
6. In this second cell, add the following code:

   ```python
   import requests, os

   url = '<unstructured-api-url>' + '/workflows/<workflow-id>/run'

   # Option 1: Get the Unstructured API key from an environment variable.
   api_key = os.getenv("UNSTRUCTURED_API_KEY")

   # Option 2: Get the Unstructured API key from Databricks Secrets.
   api_key = dbutils.secrets.get(scope="<scope>", key="<key>")

   headers = {
       'accept': 'application/json',
       'content-type': 'application/json',
       'unstructured-api-key': api_key
   }

   json_data = {}

   try:
       response = requests.post(url, headers=headers, json=json_data)
       response.raise_for_status()
       print(f'Status Code: {response.status_code}')
       print('Response:', response.json())
   except Exception as e:
       print('An error occurred:', e)
   ```

7. Replace the placeholders in this second cell as follows:

   - Replace `<unstructured-api-url>` with the value of the **Unstructured Workflow Endpoint** field from the previous step.
   - Replace `<workflow-id>` with the ID of the workflow that you want to run.
   - If you ..., 
   - If, however, you ..., replace `<scope>` and `<key>` with the scope and key names for the existing secret that you have already created in Databricks Secrets.

## Step 2: Create a job in Lakeview Jobs to run the notebook

1. With your Databricks workspace still open from the previous step, ...
2. ...