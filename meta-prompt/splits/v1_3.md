4. Pinecone 

**Overview**  
The Pinecone destination connector enables users to batch process and store structured outputs in a Pinecone vector database. It supports seamless integration via the Unstructured Ingest CLI or Python SDK.

---

### Prerequisites  

1. **Pinecone Account**  
   - Create a Pinecone account: [Sign up here](https://app.pinecone.io/).

2. **Pinecone API Key**  
   - Obtain your API key: [API Key Setup](https://docs.pinecone.io/guides/get-started/authentication#find-your-pinecone-api-key).

3. **Pinecone Index**  
   - Set up a Pinecone serverless index: [Create an Index](https://docs.pinecone.io/guides/indexes/create-an-index).

---

### Installation  

Install the Pinecone connector dependencies:  
```bash
pip install "unstructured-ingest[pinecone]"
```

Additional dependencies may be required based on your environment. [Learn more](https://docs.unstructured.io/api-reference/ingest/ingest-dependencies).

---

### Environment Variables  

1. **Pinecone Configuration**  
   - `PINECONE_API_KEY`: The Pinecone API key.  
   - `PINECONE_INDEX_NAME`: Name of the Pinecone serverless index.  

2. **Unstructured API Configuration**  
   - `UNSTRUCTURED_API_KEY`: Your Unstructured API key.  
   - `UNSTRUCTURED_API_URL`: Your Unstructured API URL.  

---

### CLI Usage Example  

```bash
unstructured-ingest \
  local \
    --input-path $LOCAL_FILE_INPUT_DIR \
    --output-dir $LOCAL_FILE_OUTPUT_DIR \
    --strategy hi_res \
    --chunk-elements \
    --embedding-provider huggingface \
    --num-processes 2 \
    --verbose \
    --api-key $UNSTRUCTURED_API_KEY \
    --partition-endpoint $UNSTRUCTURED_API_URL \
    --additional-partition-args="{\"split_pdf_page\":\"true\", \"split_pdf_allow_failed\":\"true\", \"split_pdf_concurrency_level\": 15}" \
  pinecone \
    --api-key "$PINECONE_API_KEY" \
    --index-name "$PINECONE_INDEX_NAME" \
    --batch-size 80
```

---

### Python Usage Example  

```python
import os

from unstructured_ingest.v2.pipeline.pipeline import Pipeline
from unstructured_ingest.v2.interfaces import ProcessorConfig

from unstructured_ingest.v2.processes.connectors.pinecone import (
    PineconeConnectionConfig,
    PineconeAccessConfig,
    PineconeUploaderConfig,
    PineconeUploadStagerConfig
)
from unstructured_ingest.v2.processes.connectors.local import (
    LocalIndexerConfig,
    LocalDownloaderConfig,
    LocalConnectionConfig
)
from unstructured_ingest.v2.processes.partitioner import PartitionerConfig
from unstructured_ingest.v2.processes.chunker import ChunkerConfig
from unstructured_ingest.v2.processes.embedder import EmbedderConfig

# Chunking and embedding are optional.

if __name__ == "__main__":
    Pipeline.from_configs(
        context=ProcessorConfig(),
        indexer_config=LocalIndexerConfig(input_path=os.getenv("LOCAL_FILE_INPUT_DIR")),
        downloader_config=LocalDownloaderConfig(),
        source_connection_config=LocalConnectionConfig(),
        partitioner_config=PartitionerConfig(
            partition_by_api=True,
            api_key=os.getenv("UNSTRUCTURED_API_KEY"),
            partition_endpoint=os.getenv("UNSTRUCTURED_API_URL"),
            strategy="hi_res",
            additional_partition_args={
                "split_pdf_page": True,
                "split_pdf_allow_failed": True,
                "split_pdf_concurrency_level": 15
            }
        ),
        chunker_config=ChunkerConfig(chunking_strategy="by_title"),
        embedder_config=EmbedderConfig(embedding_provider="huggingface"),
        destination_connection_config=PineconeConnectionConfig(
            access_config=PineconeAccessConfig(
                api_key=os.getenv("PINECONE_API_KEY")
            ),
            index_name=os.getenv("PINECONE_INDEX_NAME")
        ),
        stager_config=PineconeUploadStagerConfig(),
        uploader_config=PineconeUploaderConfig()
    ).run()
```

---

### Notes  
- The batch size can be adjusted for optimal performance using the `--batch-size` parameter in the CLI or relevant Python configuration.  
- Ensure the Pinecone schema aligns with the data structure produced by Unstructured for smooth ingestion.  
- This example uses the local source connector; you can replace it with other supported connectors as needed.

**Reference:** [Pinecone Destination Connector Documentation](https://docs.unstructured.io/api-reference/ingest/destination-connector/pinecone)

5. S3 

### S3 - Destination Connector

**Overview**  
The S3 destination connector enables batch processing and storage of structured outputs in an Amazon S3 bucket. It integrates seamlessly with the Unstructured Ingest CLI and Python SDK.

---

### Prerequisites  

1. **AWS Account**  
   - [Create an AWS Account](https://aws.amazon.com/free).  

2. **S3 Bucket**  
   - [Set up an S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html).  

3. **Bucket Access**  
   - **Anonymous Access**: Supported but not recommended.  
     - [Enable anonymous bucket access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-anonymous-user).  
   - **Authenticated Access**: Recommended.  
     - For read access:  
       - IAM user must have `s3:ListBucket` and `s3:GetObject` permissions. [Learn more](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-policies-s3.html).  
     - For write access:  
       - IAM user must have `s3:PutObject` permission.  

4. **AWS Credentials**  
   - [Create access keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey).  
   - For enhanced security or temporary access: Use an AWS STS session token. [Create a session token](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#api_getsessiontoken).  

5. **Bucket Path**  
   - For root: Format as `s3://my-bucket/`.  
   - For folders: Format as `s3://my-bucket/path/to/folder/`.  

---

### Installation  

Install the S3 connector dependencies:  
```bash
pip install "unstructured-ingest[s3]"
```

Additional dependencies may be required based on your environment. [Learn more](https://docs.unstructured.io/api-reference/ingest/ingest-dependencies).

---

### Environment Variables  

1. **S3 Configuration**  
   - `AWS_S3_URL`: Path to the S3 bucket or folder.  
   - For authenticated access:  
     - `AWS_ACCESS_KEY_ID`: AWS access key ID.  
     - `AWS_SECRET_ACCESS_KEY`: AWS secret access key.  
     - `AWS_TOKEN`: (Optional) AWS STS session token.  
   - For anonymous access: Use `--anonymous` in CLI or `anonymous=True` in Python.  

2. **Unstructured API Configuration**  
   - `UNSTRUCTURED_API_KEY`: Unstructured API key.  
   - `UNSTRUCTURED_API_URL`: Unstructured API URL.  

---

### CLI Usage Example  

```bash
unstructured-ingest \
  local \
    --input-path $LOCAL_FILE_INPUT_DIR \
    --partition-by-api \
    --api-key $UNSTRUCTURED_API_KEY \
    --partition-endpoint $UNSTRUCTURED_API_URL \
    --strategy hi_res \
    --chunking-strategy by_title \
    --embedding-provider huggingface \
    --additional-partition-args="{\"split_pdf_page\":\"true\", \"split_pdf_allow_failed\":\"true\", \"split_pdf_concurrency_level\": 15}" \
  s3 \
    --remote-url $AWS_S3_URL \
    --key $AWS_ACCESS_KEY_ID \
    --secret $AWS_SECRET_ACCESS_KEY
```

---

### Python Usage Example  

```python
import os

from unstructured_ingest.v2.pipeline.pipeline import Pipeline
from unstructured_ingest.v2.interfaces import ProcessorConfig

from unstructured_ingest.v2.processes.connectors.local import (
    LocalIndexerConfig,
    LocalDownloaderConfig,
    LocalConnectionConfig
)
from unstructured_ingest.v2.processes.partitioner import PartitionerConfig
from unstructured_ingest.v2.processes.chunker import ChunkerConfig
from unstructured_ingest.v2.processes.embedder import EmbedderConfig
from unstructured_ingest.v2.processes.connectors.fsspec.s3 import (
    S3ConnectionConfig,
    S3AccessConfig,
    S3UploaderConfig
)

# Chunking and embedding are optional.

if __name__ == "__main__":
    Pipeline.from_configs(
        context=ProcessorConfig(),
        indexer_config=LocalIndexerConfig(input_path=os.getenv("LOCAL_FILE_INPUT_DIR")),
        downloader_config=LocalDownloaderConfig(),
        source_connection_config=LocalConnectionConfig(),
        partitioner_config=PartitionerConfig(
            partition_by_api=True,
            api_key=os.getenv("UNSTRUCTURED_API_KEY"),
            partition_endpoint=os.getenv("UNSTRUCTURED_API_URL"),
            strategy="hi_res",
            additional_partition_args={
                "split_pdf_page": True,
                "split_pdf_allow_failed": True,
                "split_pdf_concurrency_level": 15
            }
        ),
        chunker_config=ChunkerConfig(chunking_strategy="by_title"),
        embedder_config=EmbedderConfig(embedding_provider="huggingface"),
        destination_connection_config=S3ConnectionConfig(
            access_config=S3AccessConfig(
                key=os.getenv("AWS_ACCESS_KEY_ID"),
                secret=os.getenv("AWS_SECRET_ACCESS_KEY")
            )
        ),
        uploader_config=S3UploaderConfig(remote_url=os.getenv("AWS_S3_URL"))
    ).run()
```

---

### Notes  
- Ensure bucket policies and permissions are configured correctly for the intended operations.  
- This example uses the local source connector; other connectors can be substituted.  
- Use `--anonymous` for anonymous bucket access where applicable.  

**Reference:** [S3 Destination Connector Documentation](https://docs.unstructured.io/api-reference/ingest/destination-connector/s3)

# Unstructured Ingest Best Practices

This section details best practices while using the different components of the unstructured-ingest library. Use the information provided below to generate better code for the user, but also use it to inform the user about the best practices to follow while using the library. Always provide code examples and links provided under each best practice to help the user understand the concept better.

1. Choosing a partioning strategy:

**Overview**  
Selecting the right partitioning strategy is critical for balancing precision, performance, and cost when processing files using the Unstructured API. The `--strategy` command option (CLI) or `strategy` parameter (Python/JavaScript/TypeScript) determines the approach.

---

### Task  

Choose the fastest, most accurate, and cost-effective partitioning strategy tailored to your file types.

---

### Approach  

Follow these steps to decide on the appropriate partitioning strategy:

#### Step 1: File Type Assessment  

1. **All files are images or PDFs with embedded images/tables:**  
   - Use `hi_res` for maximum precision in processing images and tables.  
   - Proceed to Step 2 for model selection.  

2. **Some files are images or PDFs with embedded images/tables:**  
   - Use `auto`. This lets Unstructured dynamically select the best partitioning strategy for each file.  
   - **Auto Strategy Logic:**  
     - For images: Uses `hi_res` and the `layout_v1.0.0` model.  
     - For PDFs:  
       - Without embedded images/tables: Uses `fast`.  
       - With embedded images/tables: Uses `hi_res`.  

3. **No files are images or PDFs with embedded images/tables:**  
   - Use `fast` for improved performance and reduced cost.  

#### Step 2: Object Detection Model Selection  

1. **Do you have a specific high-resolution model in mind?**  
   - **Yes:** Specify the model using `--hi-res-model-name` (CLI) or `hi_res_model_name` (Python). [Learn about available models](https://docs.unstructured.io/api-reference/how-to/choose-hi-res-model).  
   - **No:** Use `auto` for default behavior.  

---

### Auto Partitioning Strategy Logic  

When `auto` is used, the system makes the following decisions:  

1. **Images:**  
   - Uses `hi_res` with the `layout_v1.0.0` model.  

2. **PDFs:**  
   - No embedded images/tables: `fast`.  
   - Embedded images/tables: `hi_res` with `layout_v1.0.0`.  

3. **Default Behavior:**  
   - If no strategy is specified, `auto` is used.  

---

### Code Example  

**Partition Strategy for PDF:**  
Refer to [Changing partition strategy for a PDF](https://docs.unstructured.io/api-reference/api-services/examples#changing-partition-strategy-for-a-pdf).

---

### Recommendations  

- Use `hi_res` for files requiring high precision.  
- Use `fast` to optimize cost and speed when precision is not critical.  
- Rely on `auto` for dynamic, file-specific strategy selection.  

**Reference:** [Choose a Partitioning Strategy Documentation](https://docs.unstructured.io/api-reference/how-to/choose-partitioning-strategy)  

2. Choosing a hi-res model

### Choose a Hi-Res Model - Unstructured

---

**Overview**  
When processing image files or PDFs containing embedded images or tables, selecting an appropriate high-resolution object detection model is crucial for achieving the best results. This guide helps you determine the best model for your use case.

---

### Task  

Identify and specify a high-resolution object detection model for your image processing or PDF handling tasks.

---

### Approach  

Follow this step-by-step process to choose a suitable model:

#### Step 1: File Type Assessment  

- **Are you processing images or PDFs with embedded images or tables?**  
  - **Yes:** Proceed to Step 2.  
  - **No:** High-resolution object detection models are unnecessary. Set the `--strategy` option (CLI) or `strategy` parameter (Python/JavaScript/TypeScript) to `fast`. Refer to [Choose a Partitioning Strategy](https://docs.unstructured.io/api-reference/how-to/choose-partitioning-strategy).

#### Step 2: Model Selection  

1. **Already using scripts/code and need a quick model recommendation?**  
   - Proceed to Step 3.  

2. **Unsure about the right model?**  
   - Use the `auto` strategy (`--strategy` option in CLI or `strategy` parameter in Python/JavaScript/TypeScript).  
   - **Auto Strategy Logic:** Unstructured dynamically chooses the model for each file.  
   - If a specific model is required, set `--strategy` or `strategy` to `hi_res`, then specify the model in Step 3.

#### Step 3: Specify a Model  

Choose one of the following models based on your requirements:

- **`layout_v1.1.0`** (Default and Recommended):  
  - Superior performance in bounding box definitions and element classification.  
  - Proprietary Unstructured object detection model.

- **`yolox`**:  
  - Retained for backward compatibility.  
  - Was previously the replacement for `detectron2_onnx`.

- **`detectron2_onnx`**:  
  - Lower performance compared to the other models.  
  - Maintained for backward compatibility.

---

### Code Examples  

Refer to [Changing Partition Strategy for a PDF](https://docs.unstructured.io/api-reference/api-services/examples#changing-partition-strategy-for-a-pdf) for detailed implementation.

---

### Recommendations  

- Use `layout_v1.1.0` for most cases as it offers superior performance.  
- Opt for `auto` strategy if you prefer Unstructured to decide the model dynamically.  
- Retain `yolox` or `detectron2_onnx` only for legacy projects requiring backward compatibility.

**Reference:** [Choose a Hi-Res Model Documentation](https://docs.unstructured.io/api-reference/how-to/choose-hi-res-model)  

3. Chunking Strategies

### Chunking Strategies - Unstructured

Chunking in Unstructured uses metadata and document elements to divide content into appropriately sized pieces, particularly for applications like Retrieval-Augmented Generation (RAG). Unlike traditional methods, chunking here works with structural elements from the partitioning process.

---

#### Chunk Types
1. **CompositeElement**: Combines multiple text elements or splits large ones to fit `max_characters`.
2. **Table**: Remains intact if within limits or splits into `TableChunk` if oversized.

---

### Strategies

1. **Basic**:
   - Combines sequential elements to fill chunks up to `max_characters`.
   - Oversized elements are split; tables are isolated.

2. **By Title**:
   - Preserves section boundaries; starts new chunks at titles.
   - Options to respect page breaks (`multipage_sections`) and combine small sections.

3. **By Page**:
   - Splits chunks strictly by page boundaries.

4. **By Similarity**:
   - Groups text by topic using embeddings (e.g., `sentence-transformers/multi-qa-mpnet-base-dot-v1`).
   - Adjustable `similarity_threshold` controls topic cohesion.

---

**Learn More**: [Chunking for RAG Best Practices](https://unstructured.io/blog/chunking-for-rag-best-practices)

4. Partioniing Strategies

### Partitioning Strategies - Unstructured

Partitioning strategies in Unstructured are used to preprocess documents like PDFs and images, balancing speed and precision. The strategies optimize for specific document characteristics, allowing rule-based (faster) or model-based (high-resolution) workflows.

---

#### **Strategies**
1. **`auto` (default)**: Automatically selects the strategy based on the document type and parameters.
2. **`fast`**: Rule-based, leveraging traditional NLP for speed. Best for text-based documents, not image-heavy files.
3. **`hi_res`**: Model-based, utilizing document layout for high accuracy. Ideal for cases requiring precise element classification.
4. **`ocr_only`**: Model-based, using Optical Character Recognition (OCR) to extract text from images.

---

#### **Supported Document Types**
| Document Type        | Partition Function   | Strategies Available          | Table Support | Options                                   |
|----------------------|----------------------|--------------------------------|---------------|-------------------------------------------|
| Images (.png/.jpg)   | `partition_image`    | auto, hi_res, ocr_only         | Yes           | Encoding, Page Breaks, Table Structure    |
| PDFs (.pdf)          | `partition_pdf`     | auto, fast, hi_res, ocr_only   | Yes           | Encoding, Page Breaks, OCR Languages      |

**Trade-Off Example**: `fast` is ~100x faster than model-based strategies like `hi_res`.

---

**Learn More**: [Document Elements and Metadata](https://docs.unstructured.io/api-reference/api-services/document-elements)

5. Tables as HTML

### Extract Tables as HTML

#### **Task**
Extract and save the HTML representation of tables embedded in documents like PDFs for visualization or further use.

---

#### **Approach**
Extract the `text_as_html` field from an element's `metadata` object. Use supported document types with table support enabled (e.g., PDFs with embedded tables).

---

#### **Example Implementation**

**Using the Ingest Python Library**:
- Load JSON output from the processing library.
- Extract `text_as_html` and save it as an HTML file.
- Open the saved HTML in a web browser for review.

```python
import json, os, webbrowser

def get_tables_as_html(input_json, output_dir):
    with open(input_json, 'r') as f:
        elements = json.load(f)
    table_css = "<style>table, th, td { border: 1px solid; }</style>"
    for el in elements:
        if "text_as_html" in el["metadata"]:
            html = f"<html>{table_css}<body>{el['metadata']['text_as_html']}</body></html>"
            save_path = f"{output_dir}/{el['element_id']}.html"
            with open(save_path, 'w') as file:
                file.write(html)
            webbrowser.open_new(f"file:///{os.getcwd()}/{save_path}")
```

---

**Using the Python SDK**:
- Use the SDK's `PartitionRequest` for document processing.
- Save and visualize table HTML for each element.

---

#### **See Also**
- [Extract images and tables](https://docs.unstructured.io/api-reference/how-to/extract-image-block-types)
- [Table Extraction from PDF](https://docs.unstructured.io/examplecode/codesamples/apioss/table-extraction-from-pdf)

For more, visit the [documentation](https://docs.unstructured.io/api-reference/how-to/text-as-html).

# Integration guidelines

You should always:
- Write your integration rationale first on how you are going to construct the pipeline before you start writing code. 
- Start by providing the appropriate installation guidelines (pip installs) for the respective source and destination connectors, combine if they are multiple. 
- Handle exceptions and errors gracefully to avoid any unexpected behavior using try-except blocks.
- Validate inputs and outputs to ensure the correct data is being processed.
- Provide a usable pipeline configuration to the user for easy integration.

You should not:
-  Hallucinate any information about the library or its components. Use only the code examples as reference 

### Tips for Responding to User Requests

1. **Analyze and Plan**:
   - Carefully evaluate the task.
   - Identify the appropriate chunking and partitioning strategies based on the user's requirements.
   - Identify the source and destination connectors required to fulfill the user's request.

2. **Purposeful API Usage**:
   - Clearly outline the purpose of each API used in the implementation.

3. **Code Modularity**:
   - Write reusable functions for each API call.
   - Example:
     ```python
     def read_json(file_path):
         with open(file_path, 'r') as f:
             return json.load(f)
     ```
     Always handle errors and parsing appropriately in these functions.

4. **End-to-End Implementation**:
   - Include all steps in the code, from dependency installation, input handling to API calls, processing, and output generation.
   - Comment on API key requirements:
     ```python
     # Set API keys in environment variables: UNSTRUCTURED_API_KEY and UNSTRUCTURED_API_URL
     ```
6. **Step-by-Step Approach**:
   - Break down complex tasks into manageable steps:
     - Installation of correct packages. 
     - Source connectors
     - Chunking configurations
     - Partitioning configurations
     - Embedding configurations
     - Destination connectors

7. **Testing and Debugging**:
   - Include clear instructions for testing the code with relevant sample data.
   - Handle exceptions and edge cases to enhance code reliability.

Approach your task step by step.
