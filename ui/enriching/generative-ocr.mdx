---
title: Generative OCR optimization
---

After partitioning, you can have a vision language model (VLM) optimize the fidelity of text blocks that Unstructured 
initially processed during its partitioning phase. 

Here are a few examples of Unstructured's output of text blocks that were initially processed, and the more accurate 
version of these text blocks that were optimized by using Claude Sonnet 4. Irrelevant lines of output have been omitted here for brevity. 

Example 1: Vertical watermarked text

![Vertical watermarked text example](/img/ui/enriching/generative-ocr-watermark.png)

Before (vertical watermarked text, represented incorrectly):

```json
{
    "...": "...",
    "text": "3 2 0 2 t c O 9 2 ] V C . s c [ 2 v 9 0 8 6 1 . 0 1 3 2 : v i X r",
    "...": "..."
}
```

After (vertical watermarked text, now represented correctly from the original content):

```json
{
    "...": "...",
    "text": "arXiv:2310.16809v2 [cs.CV] 29 Oct 2023",
    "...": "..."
}
```

Example 2: Hyperlink

![Hyperlinked text example](/img/ui/enriching/generative-ocr-hyperlink.png)

Before (hyperlink, represented incorrectly):

```json
{
    "...": "...",
    "text": "con/Yuliang-Liu/MultinodalOCR|",
    "...": "..."
}
```

After (hyperlink, now represented correctly from the original content):

```json
{
    "...": "...",
    "text": "https://github.com/Yuliang-Liu/MultimodalOCR",
    "...": "..."
}
```

Example 3: Chinese characters

![Chinese characters example](/img/ui/enriching/generative-ocr-image.png)

Before (Chinese characters, represented incorrectly):

```json
{
    "...": "...",
    "text": "GT SHE GPT4-V: EHES",
    "...": "..."
}
```

After (Chinese characters, now represented correctly from the original content, expressed as Unicode):

```json
{
    "...": "...",
    "text": "GT : \u91d1\u724c\u70e7\u814a GPT4-V: \u6587\u9759\u5019\u9e1f",
    "...": "..."
}
```

## Improve text fidelity with generative OCR

import EnrichmentOCRHiResOnly from '/snippets/general-shared-text/enrichment-ocr-high-res-only.mdx';

To produce generative OCR optimizations, in an **Enrichment** node in a workflow, click the following 
in the node's settings pane's **Details** tab:
      
- **Image** under **Input Type**.
- One of the following providers and models:

  - **Anthropic** under **Provider** and any choice under **Model**
  - **OpenAI** under **Provider** and any choice under **Model**

- **Generative OCR** under **Task**.

<Warning>
    Generative OCR does not process any text blocks by default. You must also explicitly specify which document element 
    types containing text that you want generative OCR to process. To do this, in the workflow editor for your workflow:
    
    1. Click the **Partitioner** node.
    2. In the node's settings pane, scroll down to and then click a blank area inside of the **Extract Image Block Types** list.
    3. Select each [document element types](/ui/document-elements#element-type) that you want generative OCR to process. For this 
        walkthrough, select only **NarrativeText**.
    
    Generative OCR does not process the text of any `Image` or `Table` elements if they have already been processed by 
    [image description](#image-description-task) or [table description](#table-description-task) enrichments, respectively. Do 
    not remove the **Image** or **Table** document elements types from this **Extract Image Block Types** list, or else 
    the image description and table description enrichments in your workflow might produce unexepcted results or might not work at all.
</Warning>

<Note>
    The **Generative OCR** enrichment appears under the **Input Type** of **Image**, even though this is not an image-related enrichment. 
    This is a known issue and will be addressed in a future release. 
</Note>

<Note>
    You can change a workflow's image description settings only through [Custom](/ui/workflows#create-a-custom-workflow) workflow settings.

    For workflows that use [chunking](/ui/chunking), the **Chunker** node should be placed after all **Enrichment** nodes. Placing the 
    **Chunker** node before an image descriptions **Enrichment** node could cause incomplete or no image descriptions to be generated.
</Note>

<EnrichmentOCRHiResOnly />
